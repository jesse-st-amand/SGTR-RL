"""Load and parse evaluation files from self-rec-framework.

This module handles reading inspect_ai .eval files and extracting
samples for DPO training.
"""

from dataclasses import dataclass
from pathlib import Path
from typing import Optional
import re

from inspect_ai.log import read_eval_log, EvalSample as InspectEvalSample


@dataclass
class EvalSample:
    """Parsed evaluation sample from inspect_ai eval file.

    Attributes:
        sample_id: Unique identifier for this sample
        prompt: The full prompt given to the model
        reasoning: Chain-of-thought reasoning generated by model
        answer: Final answer token ("1" or "2")
        target: Ground truth answer ("1" or "2")
        is_correct: Whether answer matches target
        evaluator_model: Model that performed evaluation
        generator_model: Model that generated the text being evaluated
        dataset: Dataset name (e.g., "wikisum")
        experiment: Experiment name from directory
        metadata: Additional metadata from eval file
    """

    sample_id: str
    prompt: str
    reasoning: str
    answer: str
    target: str
    is_correct: bool
    evaluator_model: str
    generator_model: str
    dataset: str
    experiment: str
    metadata: dict


def extract_reasoning_and_answer(sample: InspectEvalSample) -> tuple[str, str]:
    """Extract reasoning and final answer from inspect_ai sample.

    Args:
        sample: InspectEvalSample from eval log

    Returns:
        tuple: (reasoning_text, answer_token)
    """
    if not sample.messages or len(sample.messages) < 2:
        return "", ""

    # Get assistant's response (last message)
    assistant_msg = sample.messages[-1]

    if not hasattr(assistant_msg, "content") or not assistant_msg.content:
        return "", ""

    reasoning = ""
    answer = ""

    # Extract from content (list of ContentReasoning and ContentText)
    for content in assistant_msg.content:
        if hasattr(content, "type"):
            if content.type == "reasoning" and hasattr(content, "reasoning"):
                reasoning = content.reasoning
            elif content.type == "text" and hasattr(content, "text"):
                answer = content.text.strip()

    return reasoning, answer


def parse_eval_filename(filename: str) -> dict[str, str]:
    """Parse evaluator and generator models from eval filename.

    Expected format:
    YYYY-MM-DDTHH-MM-SS+00-00_{evaluator}-eval-on-{generator}-treatment_{uuid}.eval

    Args:
        filename: Eval filename

    Returns:
        dict with 'evaluator' and 'generator' keys

    Example:
        >>> parse_eval_filename("2026-01-24T12-28-18+00-00_qwen-3.0-80b-thinking-eval-on-deepseek-r1-thinking-treatment_abc123.eval")
        {'evaluator': 'qwen-3.0-80b-thinking', 'generator': 'deepseek-r1-thinking'}
    """
    # Pattern: {timestamp}_{evaluator}-eval-on-{generator}-treatment_{uuid}.eval
    pattern = r"(\d{4}-\d{2}-\d{2}T[\d\-:+]+)_(.+?)-eval-on-(.+?)-treatment_([^_]+)\.eval"
    match = re.match(pattern, filename)

    if not match:
        raise ValueError(f"Could not parse eval filename: {filename}")

    timestamp, evaluator, generator, uuid = match.groups()

    return {
        "timestamp": timestamp,
        "evaluator": evaluator,
        "generator": generator,
        "uuid": uuid,
    }


def load_eval_file(
    eval_path: Path,
    dataset: Optional[str] = None,
    experiment: Optional[str] = None,
) -> list[EvalSample]:
    """Load and parse a single eval file.

    Args:
        eval_path: Path to .eval file
        dataset: Dataset name (inferred from path if not provided)
        experiment: Experiment name (inferred from path if not provided)

    Returns:
        list[EvalSample]: Parsed evaluation samples

    Example:
        >>> samples = load_eval_file(Path("path/to/file.eval"))
        >>> print(f"Loaded {len(samples)} samples")
    """
    # Infer dataset and experiment from path if not provided
    if dataset is None or experiment is None:
        parts = eval_path.parts
        if "wikisum" in parts:
            dataset = dataset or "wikisum"
        elif "bigcodebench" in parts:
            dataset = dataset or "bigcodebench"
        elif "pku_saferlhf" in parts:
            dataset = dataset or "pku_saferlhf"
        elif "sharegpt" in parts:
            dataset = dataset or "sharegpt"

        # Experiment is usually the parent directory
        experiment = experiment or eval_path.parent.name

    # Parse filename to get evaluator/generator info
    filename_info = parse_eval_filename(eval_path.name)
    evaluator_model = filename_info["evaluator"]
    generator_model = filename_info["generator"]

    # Read eval log
    log = read_eval_log(str(eval_path))

    parsed_samples = []

    for sample in log.samples:
        # Extract reasoning and answer
        reasoning, answer = extract_reasoning_and_answer(sample)

        # Get target (ground truth)
        target = str(sample.target) if sample.target else ""

        # Determine if correct
        is_correct = answer == target

        # Create parsed sample
        parsed_sample = EvalSample(
            sample_id=f"{filename_info['uuid']}_{sample.id}",
            prompt=sample.input or "",
            reasoning=reasoning,
            answer=answer,
            target=target,
            is_correct=is_correct,
            evaluator_model=evaluator_model,
            generator_model=generator_model,
            dataset=dataset or "unknown",
            experiment=experiment or "unknown",
            metadata=sample.metadata or {},
        )

        parsed_samples.append(parsed_sample)

    return parsed_samples


def load_experiment_evals(
    experiment_dir: Path,
    evaluator_model: Optional[str] = None,
    dataset: Optional[str] = None,
) -> list[EvalSample]:
    """Load all eval files from an experiment directory.

    Args:
        experiment_dir: Path to experiment directory containing .eval files
        evaluator_model: If provided, only load evals from this evaluator
        dataset: Dataset name (inferred from path if not provided)

    Returns:
        list[EvalSample]: All parsed samples from matching eval files

    Example:
        >>> samples = load_experiment_evals(
        ...     Path("data/cached_generations/wikisum/training_set_1-20/experiment_name"),
        ...     evaluator_model="qwen-3.0-80b-thinking"
        ... )
        >>> print(f"Loaded {len(samples)} samples from Qwen3-80B evaluations")
    """
    experiment_name = experiment_dir.name

    # Find all .eval files
    eval_files = list(experiment_dir.glob("*.eval"))

    # Filter by evaluator if specified
    if evaluator_model:
        eval_files = [
            f
            for f in eval_files
            if f"_{evaluator_model}-eval-on-" in f.name
        ]

    all_samples = []

    for eval_file in eval_files:
        try:
            samples = load_eval_file(eval_file, dataset=dataset, experiment=experiment_name)
            all_samples.extend(samples)
        except Exception as e:
            print(f"Warning: Failed to load {eval_file.name}: {e}")
            continue

    return all_samples


def categorize_sample(sample: EvalSample) -> str:
    """Categorize sample as TP, TN, FP, or FN.

    For Individual (IND) recognition:
    - TP: Model's text (target="1") → Model claims authorship (answer="1") ✓
    - TN: Other's text (target="2") → Model doesn't claim (answer="2") ✓
    - FP: Other's text (target="2") → Model claims authorship (answer="1") ✗
    - FN: Model's text (target="1") → Model doesn't claim (answer="2") ✗

    Args:
        sample: EvalSample to categorize

    Returns:
        str: "TP", "TN", "FP", or "FN"
    """
    # Determine if text was generated by evaluator model
    # In IND recognition:
    # - If evaluator == generator → target should be "1" (this is my text)
    # - If evaluator != generator → target should be "2" (this is not my text)

    # Use metadata's correct_answer if available, otherwise infer from target
    ground_truth_is_self = sample.target == "1"
    model_claims_self = sample.answer == "1"

    if ground_truth_is_self and model_claims_self:
        return "TP"  # Correctly identified own text
    elif not ground_truth_is_self and not model_claims_self:
        return "TN"  # Correctly rejected other's text
    elif not ground_truth_is_self and model_claims_self:
        return "FP"  # Incorrectly claimed other's text
    else:  # ground_truth_is_self and not model_claims_self
        return "FN"  # Incorrectly rejected own text
